{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2dmaEOfTh3pwsY4TeFEjb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qidopox/Deep_Learning_for_image_processing_practice/blob/main/DL_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Deep Learning for image processing practice"
      ],
      "metadata": {
        "id": "xY_W7Qy8mm0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Neural network (CNN) was usually used for image processing. Common applications include image classification, segmentation and denoising. In this practice, we are going to train a neural network for classifying handwritten numbers （MNIST dataset）. We are going to use two different trained neural networks for two separate tasks - image segmentation and denoising."
      ],
      "metadata": {
        "id": "0okQpRpVtaps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 1: Train a neural network for classifying handwritten numbers\n",
        "\n",
        "In this practice, we will use MNIST database. The MNIST database (Modified National Institute of Standards and Technology database) is a collection of handwritten digits. For details of the dataset, please see https://paperswithcode.com/dataset/mnist\n"
      ],
      "metadata": {
        "id": "ktQyeTeHxbY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first import the relevant python packages. In this case, we will use tensorflow and keras to build and train neural networks. The matlotlib package is for figure plotting. The datatime package is to provide access to date and time."
      ],
      "metadata": {
        "id": "FLz824OPd5yS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4OTrxxGlB8j"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST dataset was included in the keras dataset and thus can be loaded directly from keras.\n",
        "\n",
        "As tensorflow takes float32 as the input, we need to convert the input images, which are in uint8 format, to float32."
      ],
      "metadata": {
        "id": "xRfiwXj7epxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = tf.cast(x_train, tf.float32) / 255\n",
        "x_test = tf.cast(x_test, tf.float32) / 255"
      ],
      "metadata": {
        "id": "D-N-7B4iFLBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing out the sizes of the training dataset and test dataset. There are 60,000 examples in the training dataset and 10,000 in the test dataset. The image inputs are in the sizes of 28 by 28."
      ],
      "metadata": {
        "id": "jTlEHhqEfOBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('training data input shape:',x_train.shape,'\\n training data label shape',y_train.shape,'\\n test data input shape:',x_test.shape,'\\n test data label shape:',y_test.shape)"
      ],
      "metadata": {
        "id": "GStP2KLjJlLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting out an example from the training dataset and check its corresponding label is correct."
      ],
      "metadata": {
        "id": "8poDwMIDfmHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "plt.imshow(x_train[i,:,:])\n",
        "plt.show()\n",
        "print('label of the figure is ', y_train[i])"
      ],
      "metadata": {
        "id": "KlATCX2Qtpvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct a neural network with an input the same size as the images and an output equals to 10 which corresponds to the 10 different digits, 0-9."
      ],
      "metadata": {
        "id": "c8U9uDVRfkci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = inputs = keras.Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
        "x = keras.layers.Flatten()(inputs)\n",
        "x = keras.layers.Dense(128, activation=tf.nn.relu)(x)\n",
        "outputs = keras.layers.Dense(10, activation=tf.nn.softmax)(x)\n",
        "model= keras.Model(inputs=inputs, outputs=outputs,name='mnist_classification_fully_corrected')"
      ],
      "metadata": {
        "id": "5pIw3EYeM_Rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can print out a summary of the neural network."
      ],
      "metadata": {
        "id": "r9wDYzPxryfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "NNLUpdrrRz_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We select \"Adam\" as the optimisation algorithm for the neural network training. The loss function is \"SparseCategoricalCrossentropy\". We also use tensorboard to monitor the training process.\n",
        "\n",
        "For details of \"Adam\", please find https://arxiv.org/abs/1412.6980\n",
        "\n",
        "For details of \"SparseCategoricalCrossentropy\", please find https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy"
      ],
      "metadata": {
        "id": "We9OzK8Sr8tI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],)\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "metadata": {
        "id": "Ggp6HuAuRX8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We fit x_train dataset as the input training dataset and y_train as the output training dataset. We select to train a maximum of 30 epochs. We fit x_test dataset as the input validation dataset and y_test as the output validation dataset.\n",
        "\n",
        "By running this cell, you will start to train your neural network."
      ],
      "metadata": {
        "id": "CIW9zgU9uJwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    x=x_train,y=y_train,\n",
        "    epochs=30,\n",
        "    validation_data=(x_test,y_test),\n",
        "    callbacks=[tensorboard_callback],\n",
        ")"
      ],
      "metadata": {
        "id": "-mNMokx9z3Ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open tensorboard to monitor the network training."
      ],
      "metadata": {
        "id": "OZ9aWRH5Mbwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "B2qb794IN4Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The trained network model is saved in the folder \"models\" as a h5 file."
      ],
      "metadata": {
        "id": "YrfiwkNL6uAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('./models/'+model.name+'_.h5')"
      ],
      "metadata": {
        "id": "XFRKIInKuyKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the trained model. Using the trained model to classify the digit from handwritten number."
      ],
      "metadata": {
        "id": "uxijpq26QSrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model('./models/'+model.name+'_.h5', compile=False)\n",
        "i = 1\n",
        "y_pred=model.predict(x_test[i,:,:].reshape([1,28,28,1]))\n",
        "\n",
        "print('label:',y_test[i],'\\n prediction:',y_pred)"
      ],
      "metadata": {
        "id": "v4vNeQ217X1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 2: Train a convolutional neural network working on MNIST dataset for classifying handwritten numbers\n",
        "\n",
        "Let's now construct a CNN for the same task. Please compare the architectures and the performances of the two types of networks."
      ],
      "metadata": {
        "id": "8F3sa4lMUmMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(x_train.shape[1], x_train.shape[2],1))\n",
        "x = keras.layers.Conv2D(4,(3, 3),activation=tf.nn.relu,\n",
        "                  kernel_initializer=\"glorot_uniform\",\n",
        "                  padding=\"same\",name=\"conv1\",)(inputs)\n",
        "x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name=\"pool1\")(x)\n",
        "x = keras.layers.Conv2D(8,(3, 3),activation=tf.nn.relu,\n",
        "                  kernel_initializer=\"glorot_uniform\",\n",
        "                  padding=\"same\",name=\"conv2\",)(x)\n",
        "x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name=\"pool2\")(x)\n",
        "x = keras.layers.Flatten()(x)\n",
        "x = keras.layers.Dense(16, activation=tf.nn.relu)(x)\n",
        "outputs = keras.layers.Dense(10, activation=tf.nn.softmax)(x)\n",
        "model_CNN = keras.Model(inputs=inputs, outputs=outputs)\n"
      ],
      "metadata": {
        "id": "CfsdjEkh6AHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_CNN.summary()"
      ],
      "metadata": {
        "id": "dDg5roRF90eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_CNN.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],)\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "metadata": {
        "id": "fgQzkLYG93u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_CNN.fit(\n",
        "    x=x_train,y=y_train,\n",
        "    epochs=30,\n",
        "    validation_data=(x_test,y_test),\n",
        "    callbacks=[tensorboard_callback],\n",
        ")"
      ],
      "metadata": {
        "id": "jOGuAl4WAVoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Please take a look at the loss function plot (epoch_loss) on the tensorboard. What are the differences of the two plots? Why do you think the differences?\n",
        " If you repeat the process and retrain the network, do you obtain the same result plots on tensorboard?"
      ],
      "metadata": {
        "id": "Q_Uczcq541Ji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 3: Train a U-net for image segmentation through transfer learning\n",
        "\n",
        "In this practice, we use the Oxford-IIIT Pet Dataset (Parkhi et al, 2012). The dataset consists of images of 37 pet breeds. Please see details here https://www.robots.ox.ac.uk/%7Evgg/data/pets/\n",
        "\n",
        "This Exercise was adapted from the tensorflow tutorial https://www.tensorflow.org/tutorials/images/segmentation\n",
        "\n"
      ],
      "metadata": {
        "id": "DqLrWdV_Ymkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "dmckYV6FBTp6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you work on this practice in the same runtime as the first practice, you will not need to re-import the following packages."
      ],
      "metadata": {
        "id": "OQqkdYeHkzcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime"
      ],
      "metadata": {
        "id": "uZq3afGUlGt2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Oxford-IIIT Pet Dataset. It may take a while."
      ],
      "metadata": {
        "id": "zZQBqCpblNAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)"
      ],
      "metadata": {
        "id": "NxFGNYkqlMKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the data info."
      ],
      "metadata": {
        "id": "r4mNa4n0-Fao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(info)"
      ],
      "metadata": {
        "id": "Wz9FTUsjuXAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the data for batch training. There are two functions and one class we defined here:\n",
        "\n",
        "1. Normalise the data so that the pixel values of the images fall in the range of [0,1]. The pixel values in the masks for image segmentations were labeled either {1,2,3}. It is easier to work in python if the labels are {0,1,2} and thus we deduct 1 to all pixels in the masks.\n",
        "\n",
        "2. Load the images. Tensorflow dataset has a non-specified image size that allow users to define. We set the input images and masks to have sizes of 128 by 128.\n",
        "\n",
        "3. Data augmentation. We do some simple data augmentation by flip and rotate the images and masks.\n",
        "\n",
        "Finally we define training batch and test batch to feed into network training.\n",
        "\n",
        "* Note that for those who are not too familiar with python, *def* and *class* can be seen as two styles of python programming and differences were subtle. *def* is task oriented and *class* is data oriented."
      ],
      "metadata": {
        "id": "GwdYS2qx-NfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(input_image, input_mask):\n",
        "  input_image = tf.cast(input_image, tf.float32) / 255.0\n",
        "  input_mask -= 1\n",
        "  return input_image, input_mask\n",
        "\n",
        "def load_image(datapoint):\n",
        "  input_image = tf.image.resize(datapoint['image'], (128, 128))\n",
        "  input_mask = tf.image.resize(\n",
        "    datapoint['segmentation_mask'],\n",
        "    (128, 128),\n",
        "    method = tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n",
        "  )\n",
        "\n",
        "  input_image, input_mask = normalize(input_image, input_mask)\n",
        "\n",
        "  return input_image, input_mask\n",
        "\n",
        "TRAIN_LENGTH = info.splits['train'].num_examples\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n",
        "\n",
        "train_images = dataset['train'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_images = dataset['test'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "class Augment(tf.keras.layers.Layer):\n",
        "  def __init__(self, seed=(42,15,28)):\n",
        "    super().__init__()\n",
        "    # both use the same seed, so they'll make the same random changes.\n",
        "    self.augment_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed[0])\n",
        "    self.augment_labels = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed[0])\n",
        "\n",
        "    self.augment_inputs = tf.keras.layers.RandomFlip(mode=\"vertical\", seed=seed[1])\n",
        "    self.augment_labels = tf.keras.layers.RandomFlip(mode=\"vertical\", seed=seed[1])\n",
        "\n",
        "    self.augment_inputs = tf.keras.layers.RandomRotation(factor=0.5, seed=seed[2])\n",
        "    self.augment_labels = tf.keras.layers.RandomRotation(factor=0.5, seed=seed[2])\n",
        "\n",
        "\n",
        "  def call(self, inputs, labels):\n",
        "    inputs = self.augment_inputs(inputs)\n",
        "    labels = self.augment_labels(labels)\n",
        "    return inputs, labels\n",
        "\n",
        "train_batches = (\n",
        "    train_images\n",
        "    .cache()\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .repeat()\n",
        "    .map(Augment())\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "\n",
        "test_batches = test_images.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "tpt1MdfNuiC2"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's take a look of the training input images and their corresponding masks."
      ],
      "metadata": {
        "id": "DnRQ-yY9bsK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display(display_list):\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
        "\n",
        "  for i in range(len(display_list)):\n",
        "    plt.subplot(1, len(display_list), i+1)\n",
        "    plt.title(title[i])\n",
        "    plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
        "    plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "for images, masks in train_batches.take(10):\n",
        "  sample_image, sample_mask = images[0], masks[0]\n",
        "  display([sample_image, sample_mask])"
      ],
      "metadata": {
        "id": "cyHplkTEHZif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pa6BFohKHeFb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}